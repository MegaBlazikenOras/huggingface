{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-29T15:32:44.399906Z","iopub.execute_input":"2022-01-29T15:32:44.400333Z","iopub.status.idle":"2022-01-29T15:32:44.410016Z","shell.execute_reply.started":"2022-01-29T15:32:44.400287Z","shell.execute_reply":"2022-01-29T15:32:44.408725Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\nimport torch\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2022-01-29T15:33:12.135402Z","iopub.execute_input":"2022-01-29T15:33:12.136137Z","iopub.status.idle":"2022-01-29T15:33:12.141056Z","shell.execute_reply.started":"2022-01-29T15:33:12.136084Z","shell.execute_reply":"2022-01-29T15:33:12.140180Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#default pipeline with default model\nclassifier = pipeline(\"sentiment-analysis\")","metadata":{"execution":{"iopub.status.busy":"2022-01-29T15:33:55.604958Z","iopub.execute_input":"2022-01-29T15:33:55.605790Z","iopub.status.idle":"2022-01-29T15:34:09.154896Z","shell.execute_reply.started":"2022-01-29T15:33:55.605750Z","shell.execute_reply":"2022-01-29T15:34:09.153911Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Simple classification\nresult = classifier('I am happy while write this notebook')\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T15:35:31.338705Z","iopub.execute_input":"2022-01-29T15:35:31.339149Z","iopub.status.idle":"2022-01-29T15:35:31.462700Z","shell.execute_reply.started":"2022-01-29T15:35:31.339096Z","shell.execute_reply":"2022-01-29T15:35:31.461974Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Simple classification but with lists of strings\nresult = classifier(['I am happy while write this notebook','However I was sad yesterday'])\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T15:36:24.785913Z","iopub.execute_input":"2022-01-29T15:36:24.786506Z","iopub.status.idle":"2022-01-29T15:36:25.052104Z","shell.execute_reply.started":"2022-01-29T15:36:24.786454Z","shell.execute_reply":"2022-01-29T15:36:25.051214Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# We are done with default pipeline with default model\n# Now lets use concreate model and concreate tokenizer\n\n# now we are using distil model with small and faster version of BERT but pretrained on same corpus\n# It was also finetuned and sst-2-english is name of the dataset ( english dataset from stanford sentiment tree bank V2)\nmodel_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\nclassifier = pipeline(\"sentiment-analysis\",model=model_name)\nresult = classifier(['I am happy while write this notebook','However I was sad yesterday'])\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T15:44:39.202960Z","iopub.execute_input":"2022-01-29T15:44:39.203238Z","iopub.status.idle":"2022-01-29T15:44:44.865328Z","shell.execute_reply.started":"2022-01-29T15:44:39.203206Z","shell.execute_reply":"2022-01-29T15:44:44.864358Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# AutoTokenizer -> Generic ; AutoModelForSequenceClassification -> Specific\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification","metadata":{"execution":{"iopub.status.busy":"2022-01-29T15:50:20.383185Z","iopub.execute_input":"2022-01-29T15:50:20.383461Z","iopub.status.idle":"2022-01-29T15:50:20.387553Z","shell.execute_reply.started":"2022-01-29T15:50:20.383434Z","shell.execute_reply":"2022-01-29T15:50:20.386709Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nclassifier = pipeline(\"sentiment-analysis\",model=model, tokenizer=tokenizer)\nresult = classifier(['I am happy while write this notebook','However I was sad yesterday'])\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T15:51:36.054370Z","iopub.execute_input":"2022-01-29T15:51:36.054682Z","iopub.status.idle":"2022-01-29T15:51:41.300218Z","shell.execute_reply.started":"2022-01-29T15:51:36.054650Z","shell.execute_reply":"2022-01-29T15:51:41.299049Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Lets see tokenizer do\n# 1st way\ntokens = tokenizer.tokenize(\"I am happy while write this notebook\")\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n# 2nd way\ninput_ids = tokenizer(\"I am happy while write this notebook\")\n\nprint(f'Tokens: {tokens}')\n# ids are unique numerical representations of all words\nprint(f'Token Ids: {token_ids}')\n# same but with beginnning of string and ending of string\nprint(f'Input Ids: {input_ids}')","metadata":{"execution":{"iopub.status.busy":"2022-01-29T15:57:54.592874Z","iopub.execute_input":"2022-01-29T15:57:54.593152Z","iopub.status.idle":"2022-01-29T15:57:54.600537Z","shell.execute_reply.started":"2022-01-29T15:57:54.593124Z","shell.execute_reply":"2022-01-29T15:57:54.599548Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"Xtrain = ['I am happy while write this notebook','However I was sad yesterday']\n\nbatch = tokenizer(Xtrain,padding=True,truncation=True,max_length=512,return_tensors=\"pt\")# pt means pytorch\n\nwith torch.no_grad():\n    outputs = model(**batch) # ** used with pytorch # want the matrixes\n    print(outputs)\n    predictions = F.softmax(outputs.logits ,dim=1) # tensors\n    print(predictions)\n    labels = torch.argmax(predictions, dim=1) # in 1 or 0\n    print(labels)\n    labels = [model.config.id2label[label_id] for label_id in labels.tolist()]# list comprehension #id2label is only available for SequenceClassification\n    print(labels) # 'POSITIVE' or 'NEGATIVE'","metadata":{"execution":{"iopub.status.busy":"2022-01-29T16:18:33.457372Z","iopub.execute_input":"2022-01-29T16:18:33.457633Z","iopub.status.idle":"2022-01-29T16:18:33.506262Z","shell.execute_reply.started":"2022-01-29T16:18:33.457606Z","shell.execute_reply":"2022-01-29T16:18:33.505415Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# If we have a loss we want to inspect\nXtrain = ['I am happy while write this notebook','However I was sad yesterday']\n\nbatch = tokenizer(Xtrain,padding=True,truncation=True,max_length=512,return_tensors=\"pt\")# pt means pytorch\n\nwith torch.no_grad():\n    outputs = model(**batch,labels=torch.tensor([1,0])) # ** used with pytorch # want the matrixes \n    # label argument for loss is probablt available only for SequenceClassification\n    print(outputs)\n    predictions = F.softmax(outputs.logits ,dim=1) # tensors\n    print(predictions)\n    labels = torch.argmax(predictions, dim=1) # in 1 or 0\n    print(labels)\n    labels = [model.config.id2label[label_id] for label_id in labels.tolist()]# list comprehension #id2label is only available for SequenceClassification\n    print(labels) # 'POSITIVE' or 'NEGATIVE'\n    \n# So basically using pipeline or using doing our selves gives us the same output.\n# using model and tokenizer for manual work is important when we want to fine tune our model","metadata":{"execution":{"iopub.status.busy":"2022-01-29T16:22:28.773772Z","iopub.execute_input":"2022-01-29T16:22:28.774386Z","iopub.status.idle":"2022-01-29T16:22:28.822592Z","shell.execute_reply.started":"2022-01-29T16:22:28.774338Z","shell.execute_reply":"2022-01-29T16:22:28.821601Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Now to save our 'finetuned' model and tokenizer\n\n#save_directory = \"saved\"\n#tokenizer = AutoTokenizer.from_pretrained(save_directory)\n#model.save_pretrained(save_directory)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T16:33:42.495518Z","iopub.execute_input":"2022-01-29T16:33:42.495919Z","iopub.status.idle":"2022-01-29T16:33:42.499008Z","shell.execute_reply.started":"2022-01-29T16:33:42.495889Z","shell.execute_reply":"2022-01-29T16:33:42.498418Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Now to load our 'finetuned' model and tokenizer\n\n#save_directory = \"saved\"\n#tokenizer = AutoTokenizer.from_pretrained(save_directory)\n#model = AutoModelForSequenceClassication.from_pretrained(save_directory)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T16:33:42.688032Z","iopub.execute_input":"2022-01-29T16:33:42.688301Z","iopub.status.idle":"2022-01-29T16:33:42.691868Z","shell.execute_reply.started":"2022-01-29T16:33:42.688274Z","shell.execute_reply":"2022-01-29T16:33:42.691138Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Now lets use a different model\n# We can load a model from a local directory if available \n# Or go to https://huggingface.co/models\n# Here is modelhub where we can search for difference models\n# We can filter thorugh Tasks, Libraries, Datasets, Languages, Licenses","metadata":{"execution":{"iopub.status.busy":"2022-01-29T16:40:34.202073Z","iopub.execute_input":"2022-01-29T16:40:34.202368Z","iopub.status.idle":"2022-01-29T16:40:34.206872Z","shell.execute_reply.started":"2022-01-29T16:40:34.202337Z","shell.execute_reply":"2022-01-29T16:40:34.205850Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"#German sentiment analysis\nmodel_name = \"oliverguhr/german-sentiment-bert\"\n \nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ntexts = [\"Mit keinem guten Ergebnis\",\"Das war gut!\",\"Sie fahrt ein grunes Auto.\"]\n\nbatch = tokenizer(texts,padding=True,truncation=True,max_length=512,return_tensors=\"pt\")\n\nprint(batch) \nwith torch.no_grad():\n    outputs = model(**batch)\n    print(outputs)\n    label_ids = torch.argmax( outputs.logits,dim=1 )\n    \n    \n    labels = [model.config.id2label[label_id] for label_id in  label_ids.tolist()]\n    print(labels)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T16:54:50.842107Z","iopub.execute_input":"2022-01-29T16:54:50.842463Z","iopub.status.idle":"2022-01-29T16:54:56.251759Z","shell.execute_reply.started":"2022-01-29T16:54:50.842426Z","shell.execute_reply":"2022-01-29T16:54:56.250935Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# if we don't use pt it will return simple lists which we can convert into tensors by torch.tensor(batch['input_ids'])\n# By doing this we won't have unpack using **\n\n#German sentiment analysis\nmodel_name = \"oliverguhr/german-sentiment-bert\"\n \nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ntexts = [\"Mit keinem guten Ergebnis\",\"Das war gut!\",\"Sie fahrt ein grunes Auto.\"]\n\nbatch = tokenizer(texts,padding=True,truncation=True,max_length=512)\nbatch = torch.tensor(batch[\"input_ids\"])\nprint(batch) \nwith torch.no_grad():\n    outputs = model(batch)\n    print(outputs)\n    label_ids = torch.argmax( outputs.logits,dim=1 )\n    \n    \n    labels = [model.config.id2label[label_id] for label_id in  label_ids.tolist()]\n    print(labels)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T16:55:26.523813Z","iopub.execute_input":"2022-01-29T16:55:26.524424Z","iopub.status.idle":"2022-01-29T16:55:31.790975Z","shell.execute_reply.started":"2022-01-29T16:55:26.524386Z","shell.execute_reply":"2022-01-29T16:55:31.790343Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}